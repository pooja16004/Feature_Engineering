{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSSETZA72r5Cz6xHpF8aws"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1.What is a parameter?\n","- A parameter is a fixed, numerical value that describes a characteristic of a population in statistics or a model in mathematics.\n","\n","In Statistics:\n","A parameter is:\n","\n","A value that summarizes information about an entire population (not just a sample).\n","\n","Examples:\n","\n","Population mean (\n","ùúá\n","Œº)\n","\n","Population standard deviation (\n","ùúé\n","œÉ)\n","\n","Population proportion (\n","ùëù\n","p)\n","\n","For example, if you're measuring the average height of all adults in a country, the true average height is a parameter.\n","\n","In Mathematical Models:\n","A parameter is a constant in an equation or function that defines the system but doesn't vary with the input variables.\n","\n","For example, in the linear model\n","ùë¶\n","=\n","ùëö\n","ùë•\n","+\n","ùëè\n","y=mx+b, the values\n","ùëö\n","m (slope) and\n","ùëè\n","b (intercept) are parameters."],"metadata":{"id":"ZNiu2gej8_f9"}},{"cell_type":"markdown","source":["2.What is correlation?\n","- Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n","\n","#What does negative correlation mean?\n","-Negative correlation means that as one variable increases, the other decreases, and vice versa.\n","\n","In simple terms:\n","When two variables move in opposite directions, they have a negative correlation.\n","\n","Example:\n","Temperature vs. Hot Chocolate Sales:\n","As temperature goes up, hot chocolate sales go down."],"metadata":{"id":"V1WhZqaN9A_m"}},{"cell_type":"markdown","source":["3.Define Machine Learning. What are the main components in Machine Learning?\n","- Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn from data and make decisions or predictions without being explicitly programmed. Instead of using fixed rules, ML systems identify patterns in data and use them to make informed decisions.\n","\n","#Main Components of Machine Learning:\n","\n","#Data:\n","\n","The foundational input for ML models.\n","\n","Can be structured (e.g., tables) or unstructured (e.g., images, text).\n","\n","#Model:\n","\n","A mathematical representation that maps input data to output predictions.\n","\n","Examples: decision trees, neural networks, linear regression models.\n","\n","#Algorithm:\n","\n","The procedure or method used to train the model on data.\n","\n","Examples:\n","Gradient Descent, k-Means, Backpropagation.\n","\n","#Training:\n","\n","The process of feeding data to a model and adjusting it to minimize prediction errors.\n","\n","Involves splitting data into training, validation, and testing sets.\n","\n","#Features:\n","\n","The input variables used to make predictions.\n","\n","Feature selection and engineering are key to improving model performance.\n","\n","#Labels (for supervised learning):\n","\n","The target or output values the model tries to predict.\n","\n","Used during training to guide the learning process.\n","\n","#Evaluation:\n","\n","Assessing the model's performance using metrics like accuracy, precision, recall, or mean squared error.\n","\n","#Inference:\n","\n","Using the trained model to make predictions on new, unseen data."],"metadata":{"id":"wbi7aXx_9BCx"}},{"cell_type":"markdown","source":["4. How does loss value help in determining whether the model is good or not?\n","- The loss value is a key metric used during model training to measure how well (or poorly) a model is performing. Here's how it helps in determining whether the model is good or not:\n","#1. Definition of Loss\n","\n","The loss is a numerical value that represents the difference between the predicted output and the actual target (ground truth).\n","Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy Loss for classification.\n","#2. Interpreting the Loss Value\n","A lower loss means the model‚Äôs predictions are closer to the true values, which generally indicates better performance.\n","A high loss suggests the model is making large errors in its predictions.\n","#3. Use in Model Evaluation\n","During training, the model adjusts its internal parameters to minimize the loss.\n","Tracking the training loss and validation loss over epochs helps in:\n","Detecting underfitting (high training and validation loss).\n","Detecting overfitting (low training loss but high validation loss).\n","Evaluating convergence (loss plateaus after some epochs).\n","#4. Important Notes\n","Loss is not the same as accuracy or performance: A model can have low loss but still not be practically useful if the loss function doesn‚Äôt align well with the real-world goal.\n","Always use loss along with evaluation metrics (e.g., accuracy, precision, F1-score) for a complete picture."],"metadata":{"id":"nB3yM1iz9Bf-"}},{"cell_type":"markdown","source":["5. What are continuous and categorical variables?\n","- Continuous and categorical variables are two fundamental types of variables used in statistics and machine learning.\n","#Continuous Variables\n","Definition: Variables that can take any numeric value within a range.\n","\n","#Characteristics:\n","\n","Measurable.\n","\n","Infinite number of possible values (within a range).\n","\n","Can be fractional or decimal.\n","\n","#Examples:\n","\n","Height (e.g., 170.2 cm)\n","\n","Weight (e.g., 65.5 kg)\n","\n","Temperature (e.g., 98.6¬∞F)\n","\n","Income (e.g., $42,378.45)\n","\n","#Categorical Variables\n","Definition: Variables that represent categories or groups.\n","\n","#Characteristics:\n","\n","Values are labels or categories.\n","\n","Usually finite and discrete.\n","\n","Can be nominal (no order) or ordinal (ordered).\n","\n","#Examples:\n","\n","Nominal: Gender (Male, Female), Color (Red, Blue)\n","\n","Ordinal: Education Level (High School < Bachelor < Master), Rating (Poor, Fair, Good, Excellent)"],"metadata":{"id":"bbXkUzv79Bps"}},{"cell_type":"markdown","source":["6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n","- Handling categorical variables is crucial in machine learning because most algorithms require numerical input. Here are the common techniques to handle them:\n","#1. Label Encoding\n","What it does: Converts each category into a unique integer.\n","\n","#Example:\n","\n","Red ‚Üí 0\n","\n","Green ‚Üí 1\n","\n","Blue ‚Üí 2\n","\n","#When to use:\n","\n","When the categories are ordinal (i.e., have a meaningful order).\n","\n","#Caution:\n","\n","For nominal (unordered) data, label encoding may mislead the model into thinking there's an order.\n","\n","#2. One-Hot Encoding\n","What it does: Creates a binary column for each category.\n","\n","#Example (Color):\n","\n","Red ‚Üí [1, 0, 0]\n","Green ‚Üí [0, 1, 0]\n","Blue ‚Üí [0, 0, 1]\n","#When to use:\n","\n","For nominal variables (unordered categories).\n","\n","#Caution:\n","\n","Can lead to high dimensionality if there are many categories (curse of dimensionality).\n","\n","#3. Ordinal Encoding\n","What it does: Assigns ordered integers to categories.\n","\n","#Example:\n","\n","Low ‚Üí 1\n","\n","Medium ‚Üí 2\n","\n","High ‚Üí 3\n","\n","#When to use:\n","\n","For ordinal categorical variables where order matters.\n","\n","#4. Target Encoding (Mean Encoding)\n","What it does: Replaces each category with the mean of the target variable for that category.\n","\n","#Example (for a binary target):\n","\n","City A ‚Üí 0.7 (70% churn)\n","\n","City B ‚Üí 0.2\n","\n","#When to use:\n","\n","With high-cardinality categorical variables.\n","\n","#Caution:\n","\n","Prone to overfitting, especially with small categories. Regularization or smoothing is often needed.\n","\n","#5. Frequency Encoding\n","What it does: Encodes each category with its frequency in the dataset.\n","\n","#Example:\n","\n","Category A appears 100 times ‚Üí 100\n","\n","#When to use:\n","\n","When you suspect frequency correlates with target behavior."],"metadata":{"id":"2cnbW7tM9BsH"}},{"cell_type":"markdown","source":["7.What do you mean by training and testing a dataset?\n","- Training and testing a dataset refers to splitting your data into two separate parts for building and evaluating a machine learning model.\n","\n","#1. Training Dataset\n","Purpose: Used to train the model ‚Äî that is, to allow the model to learn the patterns and relationships in the data.\n","\n","What happens:\n","\n","The model adjusts its internal parameters based on the input features and target outputs.\n","\n","Loss function is minimized on this set.\n","#2. Testing Dataset\n","Purpose: Used to evaluate how well the trained model performs on unseen data.\n","\n","What happens:\n","\n","The model makes predictions on this data.\n","\n","Performance metrics (e.g., accuracy, precision, RMSE) are calculated.\n","\n","Helps detect overfitting (if the model performs well on training data but poorly on test data)."],"metadata":{"id":"8HZ9QeDEM1xM"}},{"cell_type":"markdown","source":["8. What is sklearn.preprocessing?\n","- sklearn.preprocessing is a module in the scikit-learn library that provides tools to prepare and transform data before feeding it into a machine learning model."],"metadata":{"id":"hiJ7PxEhM1za"}},{"cell_type":"markdown","source":["9. What is a Test set?\n","- A test set is a portion of your dataset that is used only to evaluate the final performance of a trained machine learning model.\n","\n","Purpose of a Test Set\n","To simulate how the model will perform on new, unseen data.\n","\n","Helps assess generalization ‚Äî whether the model has learned true patterns or just memorized the training data.\n","\n","It gives a realistic estimate of model accuracy in practical use."],"metadata":{"id":"_6EJjsbKM12V"}},{"cell_type":"markdown","source":["10. How do we split data for model fitting (training and testing) in Python?\n","- To split data for model fitting (training and testing) in Python, the most common and simple way is to use train_test_split from scikit-learn.\n","\n","#Step-by-Step Example\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","# Example dataset\n","data = pd.DataFrame({\n","    'feature1': [10, 20, 30, 40, 50],\n","    'feature2': [1, 2, 3, 4, 5],\n","    'target': [0, 1, 0, 1, 0]\n","})\n","\n","# Separate features (X) and target (y)\n","X = data[['feature1', 'feature2']]\n","y = data['target']\n","\n","# Split the data: 80% training, 20% testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","#Parameters Explained\n","X, y: Features and target.\n","\n","test_size=0.2: 20% of the data goes into the test set.\n","\n","random_state=42: Ensures the split is reproducible.\n","\n","train_size: Optional, use if you want to specify the training size directly.\n","\n","After the Split\n","Use X_train, y_train to train your model.\n","\n","Use X_test, y_test to evaluate model performance.\n","\n","#How do you approach a Machine Learning problem?\n","\n","- Approaching a Machine Learning (ML) problem systematically helps ensure reliable and effective results. Here's a structured approach used by most ML practitioners:\n","\n","#1. Understand the Problem\n","Clarify the goal: Is it classification, regression, clustering, etc.?\n","\n","Understand the domain: Talk to stakeholders or domain experts if needed.\n","\n","Define the success metric: Accuracy, F1-score, RMSE, AUC, etc.\n","#2. Gather and Explore Data\n","Collect data from sources like CSV files, databases, APIs, etc.\n","\n","Perform Exploratory Data Analysis (EDA):\n","\n","Summary statistics\n","\n","Visualizations (histograms, scatter plots, boxplots)\n","\n","Identify missing values, outliers, and imbalances\n","\n","#3. Preprocess the Data\n","Handle missing values (e.g., impute or remove)\n","\n","Encode categorical variables (e.g., one-hot encoding)\n","\n","Scale/normalize features (e.g., StandardScaler)\n","\n","Split the data into training and testing sets\n","\n","#4. Choose a Model\n","Based on the problem type:\n","\n","Classification: Logistic Regression, Random Forest, SVM, etc.\n","\n","Regression: Linear Regression, XGBoost, etc.\n","\n","Clustering: KMeans, DBSCAN\n","\n","Start with simple models first (baseline)\n","#5. Train the Model\n","Fit the model on the training data\n","\n","Use cross-validation to avoid overfitting and to tune hyperparameters\n","\n","#6. Evaluate the Model\n","Use the test set to assess model performance\n","\n","Use appropriate metrics:\n","\n","Classification: Accuracy, Precision, Recall, F1-score, ROC-AUC\n","\n","Regression: MSE, RMSE, R¬≤\n","\n","#7. Improve the Model\n","Feature engineering: Create or select better features\n","\n","Hyperparameter tuning: Use Grid Search or Random Search\n","\n","Try different models or ensemble methods\n","\n","#8. Deploy the Model (Optional)\n","Package the model using tools like Pickle, ONNX, or MLflow\n","\n","Deploy via Flask/FastAPI, or on cloud platforms (e.g., AWS, GCP, Azure)\n","\n","Monitor performance in the real world\n","#9. Maintain & Iterate\n","Collect new data\n","\n","Retrain/update the model\n","\n","Handle concept drift (when the underlying data distribution changes)"],"metadata":{"id":"D-U5pyX3M15x"}},{"cell_type":"markdown","source":["11. Why do we have to perform EDA before fitting a model to the data?\n","- Performing Exploratory Data Analysis (EDA) before fitting a model is crucial because it helps you deeply understand your dataset and avoid costly mistakes.\n","#why EDA is important:\n","\n","#1. Understand Data Quality\n","Detect missing values, duplicates, and inconsistent entries.\n","\n","Example: If 30% of a feature's values are missing, you may choose to drop or impute it.\n","\n","#2. Uncover Data Distributions\n","See how features are distributed (e.g., normal, skewed, outliers).\n","\n","Helps decide if you need scaling, transformation, or outlier treatment.\n","\n","#3. Identify Feature Relationships\n","Use correlation heatmaps, scatter plots, and pairplots.\n","\n","Detect multicollinearity (high correlation between features) that may affect certain models like linear regression.\n","#4. Detect Class Imbalance\n","Important for classification problems.\n","\n","For example, if 95% of the target is one class, accuracy can be misleading.\n","\n","#5. Choose the Right Preprocessing\n","EDA reveals whether features are categorical, ordinal, or numerical.\n","\n","Guides decisions on encoding, normalization, or feature engineering.\n","#6. Form Hypotheses\n","EDA can help you spot patterns or relationships that you may want the model to learn.\n","\n","For example, \"Higher age seems to be associated with higher income\" may suggest a useful feature interaction.\n","\n","#7. Save Time and Improve Accuracy\n","Catching data issues early prevents model misinterpretation, wasted training time, and poor performance.\n","\n","Helps build better models faster with more meaningful inputs."],"metadata":{"id":"OHBpIDZRM19Z"}},{"cell_type":"markdown","source":["12. What is correlation?\n","- Correlation is a statistical measure that describes the strength and direction of a relationship between two variables."],"metadata":{"id":"i5aivYP7M1BR"}},{"cell_type":"markdown","source":["13. What does negative correlation mean?\n","- Negative correlation means that as one variable increases, the other decreases, and vice versa. The two variables move in opposite directions.\n","#Key Points\n","Represented by a correlation coefficient (r) between -1 and 0.\n","\n","r = -1 indicates a perfect negative linear relationship.\n","\n","The closer to -1, the stronger the negative correlation.\n","\n","#Examples\n","Speed vs. Travel Time: The faster you drive, the less time it takes to reach your destination.\n","\n","Price vs. Demand: As prices increase, demand often decreases.\n","\n","Exercise vs. Body Fat: More exercise can be linked to lower body fat levels.\n","\n","#Visual Tip\n","On a scatter plot, a negative correlation looks like a downward slope ‚Äî from top-left to bottom-right.\n","\n","#Why It Matters in ML\n","Helps you identify inverse relationships between features and the target.\n","\n","Important for feature selection and interpreting model behavior."],"metadata":{"id":"fk1GvrkTM1VE"}},{"cell_type":"markdown","source":["14. How can you find correlation between variables in Python?\n","- You can find the correlation between variables in Python using Pandas, which provides a simple and powerful way to compute correlation matrices.\n","\n","#Step-by-Step Example\n","import pandas as pd\n","\n","# Sample dataset\n","data = {\n","    'height': [150, 160, 170, 180, 190],\n","    'weight': [50, 60, 65, 80, 85],\n","    'age': [25, 30, 35, 40, 45]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Calculate correlation matrix\n","correlation_matrix = df.corr()\n","print(correlation_matrix)"],"metadata":{"id":"82xOO4f9M1Xa"}},{"cell_type":"markdown","source":["15. What is causation? Explain difference between correlation and causation with an example.\n","- Causation refers to a relationship where one event directly causes another. It means that a change in one variable leads to a change in another. In simpler terms, if A causes B, then changing A will result in a change in B.\n","#Difference Between Correlation and Causation\n","Concept - Description\n","\n","Correlation - A relationship or association between two variables. They may move together, but one does not necessarily cause the other.\n","\n","Causation - A direct cause-and-effect relationship. One variable directly affects the other.\n","#Example\n","#Correlation Example:\n","A study finds that ice cream sales and drowning incidents both increase in summer.\n","\n","These two variables are correlated‚Äîthey rise and fall together.\n","\n","But eating ice cream does not cause drowning.\n","\n","The real cause is the hot weather, which leads to more people swimming and more ice cream being sold.\n","\n","#Causation Example:\n","Smoking is found to cause lung cancer.\n","\n","This is causation, because scientific evidence shows that the chemicals in cigarette smoke directly lead to cancerous changes in lung tissue."],"metadata":{"id":"uozi2SkJM1aJ"}},{"cell_type":"markdown","source":["16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n","# What is an Optimizer in Machine Learning?\n","An optimizer is an algorithm that adjusts the weights and biases of a neural network (or other model parameters) to minimize the loss function during training. The goal is to improve the model's performance by finding the optimal set of parameters that results in the lowest possible error.\n","\n","#Types of Optimizers\n","There are several optimizers in machine learning, each with different strategies to update parameters. Here are the most commonly used ones:\n","\n","#1. Gradient Descent (GD)\n","Basic idea: Update weights in the direction that reduces the loss.\n","\n","Formula:\n","\n","ùë§\n",":\n","=\n","ùë§\n","‚àí\n","ùúÇ\n","‚ãÖ\n","‚àá\n","ùêø\n","(\n","ùë§\n",")\n","w:=w‚àíŒ∑‚ãÖ‚àáL(w)\n","where:\n","\n","ùë§\n","w: weights\n","\n","ùúÇ\n","Œ∑: learning rate\n","\n","‚àá\n","ùêø\n","(\n","ùë§\n",")\n","‚àáL(w): gradient of the loss with respect to weights\n","\n","Limitation: Computes gradient over the entire dataset ‚Üí slow and not scalable.\n","\n","#Example:\n","Used when training on small datasets like MNIST, calculating the gradient over the full training set.\n","#2. Stochastic Gradient Descent (SGD)\n","Improvement over GD: Updates weights using one training example at a time.\n","\n","Faster, but introduces more noise in the updates.\n","\n","#Example:\n","Training a neural network on image data, updating weights after each image is processed.\n","\n","#3. Mini-Batch Gradient Descent\n","Combines the benefits of GD and SGD.\n","\n","Updates weights using a small batch of examples.\n","\n","Reduces noise and speeds up training.\n","\n","#Example:\n","Instead of using all 10,000 samples or just one, you use batches of 32 or 64 samples at a time.\n","#4. Momentum\n","Adds a fraction of the previous update to the current one to smooth updates and avoid oscillation.\n","\n","Helps escape local minima and speeds up convergence.\n","\n","Formula:\n","\n","ùë£\n",":\n","=\n","ùõæ\n","ùë£\n","+\n","ùúÇ\n","‚àá\n","ùêø\n","(\n","ùë§\n",")\n","ùë§\n",":\n","=\n","ùë§\n","‚àí\n","ùë£\n","v:=Œ≥v+Œ∑‚àáL(w)\n","w:=w‚àív\n","where:\n","\n","ùõæ\n","Œ≥: momentum factor (e.g., 0.9)\n","\n","#Example:\n","Used in training deep CNNs to speed up training and smooth the learning process.\n","#5. AdaGrad (Adaptive Gradient Algorithm)\n","Adjusts the learning rate individually for each parameter.\n","\n","Parameters with frequent updates get smaller learning rates over time.\n","\n","#Example:\n","Good for sparse data problems like text classification (e.g., using bag-of-words features).\n","\n","#6. RMSProp (Root Mean Square Propagation)\n","Improves AdaGrad by using an exponentially decaying average of past squared gradients.\n","\n","Helps avoid the issue of vanishing learning rates.\n","\n","#Example:\n","Often used in RNNs for time series or sequence data\n","\n","#7. Adam (Adaptive Moment Estimation)\n","Combines Momentum and RMSProp.\n","\n","Maintains moving averages of gradients and squared gradients.\n","\n","Formula (simplified):\n","ùë§\n",":\n","=\n","ùë§\n","‚àí\n","ùúÇ\n","‚ãÖ\n","ùëö\n","ùë°\n","ùë£\n","ùë°\n","+\n","ùúñ\n","w:=w‚àíŒ∑‚ãÖ\n","v\n","t\n","\n","\n","\n"," +œµ\n","m\n","t\n","\n","\n","\n","\n","where:\n","\n","ùëö\n","ùë°\n","m\n","t\n","\n"," : moving average of the gradient\n","\n","ùë£\n","ùë°\n","v\n","t\n","\n"," : moving average of squared gradient\n","\n","#Example:\n","Most popular optimizer for training deep learning models like BERT, CNNs, GANs, etc.\n","\n","#8. AdamW (Adam with Weight Decay)\n","Variant of Adam that decouples weight decay from the gradient update.\n","\n","Better for regularization and generalization.\n","\n","#Example:\n","Used in large-scale NLP models like Transformers and GPT."],"metadata":{"id":"xG9klQgIeYyK"}},{"cell_type":"markdown","source":["17. What is sklearn.linear_model ?\n","- What is sklearn.linear_model?\n","sklearn.linear_model is a module in Scikit-learn, a popular Python machine learning library. This module contains linear models for regression and classification tasks.\n","#Key Features\n","Provides efficient implementations of linear models.\n","\n","Supports both regularized and non-regularized models.\n","\n","Can be used for binary classification, multi-class classification, and regression problems."],"metadata":{"id":"X7MRKk_5eY__"}},{"cell_type":"markdown","source":["18. What does model.fit() do? What arguments must be given?\n","#What does model.fit() do in Scikit-learn?\n","The method model.fit() is used to train a machine learning model in Scikit-learn.\n","\n","#Purpose:\n","model.fit(X, y) tells the model to:\n","\n","Learn from the input data X\n","\n","Find the best parameters (e.g., weights in linear models)\n","\n","Map inputs X to outputs y by minimizing a loss function\n","\n","After calling .fit(), the model is ready to make predictions using .predict().\n","\n","#Required Arguments:\n","model.fit(X, y)\n","\n","Argument - Description\n","#X    Input features (independent variables). Shape: (n_samples, n_features)\n","#y    Target values (dependent variable). Shape: (n_samples,) or (n_samples, n_outputs)\n","\n","#Example 1: Linear Regression\n","from sklearn.linear_model import LinearRegression\n","\n","X = [[1], [2], [3], [4]]  # Features\n","y = [2, 4, 6, 8]          # Target\n","\n","model = LinearRegression()\n","model.fit(X, y)           # Train the model\n","\n","#After .fit(), you can:\n","Use .predict(X_test) to make predictions\n","\n","Access learned parameters like .coef_, .intercept_, etc."],"metadata":{"id":"vDjniAnteZB8"}},{"cell_type":"markdown","source":["19. What does model.predict() do? What arguments must be given?\n","#What does model.predict() do in Scikit-learn?\n","The method model.predict() is used to make predictions using a trained model.\n","\n","#Purpose:\n","After a model is trained using model.fit(X, y), you use model.predict(X_new) to:\n","\n","Apply the learned parameters to new input data\n","\n","Return the model‚Äôs predicted output values\n","\n","#Required Argument:\n","model.predict(X)\n","\n","#Argument   Description\n","X- New input data (features) for which predictions are required. Shape: (n_samples, n_features)\n","\n","#Example : Linear Regression\n","\n","from sklearn.linear_model import LinearRegression\n","\n","X_train = [[1], [2], [3]]\n","y_train = [2, 4, 6]\n","\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","X_test = [[4], [5]]\n","predictions = model.predict(X_test)\n","\n","print(predictions)  # Output: [8. 10.]\n","\n","#Example 2: Logistic Regression (Classification)\n","\n","from sklearn.linear_model import LogisticRegression\n","\n","X_train = [[0], [1], [2], [3]]\n","y_train = [0, 0, 1, 1]\n","\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","\n","print(model.predict([[1.5]]))  # Output: [0] or [1]"],"metadata":{"id":"0cGFYWwgeaUD"}},{"cell_type":"markdown","source":["20.What are continuous and categorical variables?\n","\n","#What are Continuous and Categorical Variables?\n","In statistics and machine learning, variables (or features) are typically classified into two main types: continuous and categorical.\n","\n","# 1. Continuous Variables\n","A continuous variable is one that can take any numerical value within a range. These values are measurable and often include decimals or fractions.\n","\n","#Characteristics:\n","Infinite possible values within a range\n","\n","Numeric\n","\n","Often derived from measurements\n","\n","# Examples:\n","Height (e.g., 170.5 cm)\n","\n","Temperature (e.g., 98.6¬∞F)\n","\n","Salary (e.g., $45,000.75)\n","\n","Age (e.g., 23.5 years)\n","\n","# 2. Categorical Variables\n","A categorical variable represents discrete groups or categories. These values are not numeric (even if they look like numbers) and typically describe qualities or characteristics.\n","\n","#Characteristics:\n","Limited number of distinct values\n","\n","Represent categories, labels, or types\n","\n","Can be nominal or ordinal\n","\n","#Examples:\n","Gender (Male, Female)\n","\n","Color (Red, Green, Blue)\n","\n","Education Level (High School, Bachelor's, Master's)\n","\n","Country (USA, Canada, India)"],"metadata":{"id":"Iix02Hx-eagH"}},{"cell_type":"markdown","source":["21. What is feature scaling? How does it help in Machine Learning?\n","\n","#What is Feature Scaling?\n","Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset.\n","\n","In simple terms, it means adjusting the values of features so that they fall within a similar scale, which helps certain machine learning algorithms perform better and converge faster.\n","\n","#Why Feature Scaling is Important in Machine Learning\n","Some algorithms are sensitive to the magnitude of input features because they rely on distance or gradient calculations. Without scaling, features with larger ranges can dominate others, leading to biased models or slower convergence.\n","\n","#Algorithms Affected by Feature Scaling\n","# Sensitive (require scaling):\n","\n","K-Nearest Neighbors (KNN)\n","\n","Support Vector Machines (SVM)\n","\n","Logistic Regression\n","\n","Linear Regression\n","\n","Gradient Descent‚Äìbased algorithms (e.g., Neural Networks)\n","\n","PCA (Principal Component Analysis)\n","\n","# Not sensitive (don't require scaling):\n","\n","Tree-based models like Decision Trees, Random Forest, XGBoost\n","\n","#Common Methods of Feature Scaling\n","#1. Min-Max Scaling (Normalization)\n","Scales values to a [0, 1] range.\n","\n","Formula:\n","\n","ùëã\n","‚Ä≤\n","=\n","ùëã\n","‚àí\n","ùëã\n","min\n","ùëã\n","max\n","‚àí\n","ùëã\n","min\n","X\n","‚Ä≤\n"," =\n","X\n","max\n","\n"," ‚àíX\n","min\n","\n","\n","X‚àíX\n","min\n","\n","\n","\n","\n","# Example:\n","Original: [10, 20, 30] ‚Üí Scaled: [0.0, 0.5, 1.0]\n","\n","#2. Standardization (Z-score Normalization)\n","Scales values to have mean = 0 and standard deviation = 1.\n","\n","Formula:\n","\n","ùëã\n","‚Ä≤\n","=\n","ùëã\n","‚àí\n","ùúá\n","ùúé\n","X\n","‚Ä≤\n"," =\n","œÉ\n","X‚àíŒº\n","\n","\n","where\n","ùúá\n","Œº is the mean, and\n","ùúé\n","œÉ is the standard deviation.\n","\n","# Example:\n","Original: [10, 20, 30] ‚Üí Scaled: [-1.22, 0, 1.22] (approximately)\n","\n","#Python Example using Scikit-learn\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","\n","# Sample data\n","X = [[10], [20], [30]]\n","\n","# Min-Max Scaling\n","min_max = MinMaxScaler()\n","print(min_max.fit_transform(X))  # Output: [[0. ], [0.5], [1. ]]\n","\n","# Standard Scaling\n","standard = StandardScaler()\n","print(standard.fit_transform(X))  # Output: [[-1.22], [0.], [1.22]]"],"metadata":{"id":"b7oXcIMNeQMt"}},{"cell_type":"markdown","source":["22. How do we perform scaling in Python?\n","\n","-How to Perform Feature Scaling in Python (with Scikit-learn)\n","Scikit-learn provides easy and efficient tools to scale features using preprocessing classes.\n","\n","#Step-by-Step\n","#1. Import Libraries\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","\n","#2. Min-Max Scaling (Normalization)\n","Scales features to a fixed range, usually [0, 1].\n","\n","# Sample data\n","X = [[10], [20], [30]]\n","\n","# Create and apply scaler\n","scaler = MinMaxScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","print(X_scaled)\n","# Output: [[0. ], [0.5], [1. ]]\n","\n","#3. Standardization (Z-score Scaling)\n","Centers features around mean = 0 and std = 1.\n","\n","# Sample data\n","X = [[10], [20], [30]]\n","\n","# Create and apply scaler\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","print(X_scaled)\n","# Output: [[-1.2247], [0.], [1.2247]] (approx.)\n","\n","#4. Scaling Multiple Features\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# Multiple features\n","X = np.array([\n","    [1, 100],\n","    [2, 200],\n","    [3, 300]\n","])\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","print(X_scaled)\n","\n","#Best Practices:\n","Always fit the scaler on the training data only:\n","\n","scaler.fit(X_train)\n","X_train_scaled = scaler.transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"],"metadata":{"id":"zfqjDbt2fJ2X"}},{"cell_type":"markdown","source":["23. What is sklearn.preprocessing?\n","#What is sklearn.preprocessing?\n","sklearn.preprocessing is a module in Scikit-learn that provides a collection of utility functions and classes for preprocessing data before training a machine learning model.\n","\n","#Purpose:\n","Data preprocessing ensures that your features are in a format that machine learning models can understand and perform well with."],"metadata":{"id":"PaI3uwZReQdr"}},{"cell_type":"markdown","source":["24.How do we split data for model fitting (training and testing) in Python?\n","\n","#How to Split Data for Model Training and Testing in Python (Scikit-learn)\n","In machine learning, it‚Äôs important to split your dataset into:\n","\n","Training data: used to train (fit) the model\n","\n","Testing data: used to evaluate the model‚Äôs performance on unseen data\n","\n","#Use train_test_split() from sklearn.model_selection\n","#Step-by-Step Example\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Example data\n","X = [[1], [2], [3], [4], [5], [6]]  # Features\n","y = [10, 20, 30, 40, 50, 60]       # Target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","print(\"Training features:\", X_train)\n","print(\"Testing features:\", X_test)\n","\n","#Why Split the Data?\n","Prevents overfitting by testing on unseen data\n","\n","Helps measure generalization performance\n","\n","Ensures fair comparison of different models\n","\n","#Real-World Tip:\n","Always fit scalers or encoders only on training data, then transform both train and test data with the same transformer to avoid data leakage."],"metadata":{"id":"EUNQ_lFXeQgD"}},{"cell_type":"markdown","source":["25.Explain data encoding?\n","-Data encoding is the process of converting data from one form to another for the purpose of transmission, storage, or processing. The goal is to represent data in a format that is usable and efficient for a specific system or context.\n","\n","#Types of Data Encoding\n","#1. Character Encoding\n","\n","Converts characters into binary values.\n","\n","#Examples:\n","\n","- ASCII (American Standard Code for Information Interchange): Encodes English characters into 7-bit binary.\n","\n","- UTF-8 (Unicode Transformation Format): Supports a wide range of characters (used widely on the web).\n","\n","#2.Binary Encoding\n","- Represents numeric or text data as binary numbers (0s and 1s).\n","\n","- Fundamental to how computers store and process data.\n","\n","#3.Data Compression Encoding\n","\n","- Reduces the size of data for storage or transmission.\n","\n","#Examples:\n","\n","- Huffman Coding\n","\n","- Run-Length Encoding (RLE)\n","\n","- ZIP, JPEG, MP3 (formats that use compression encoding)\n","\n","#4.Line Encoding (in communication systems)\n","\n","- Converts digital data into signals for transmission over a medium.\n","\n","#Examples:\n","\n","- NRZ (Non-Return to Zero)\n","\n","- Manchester Encoding\n","\n","#5.Base Encoding\n","\n","- Encodes binary data into readable formats.\n","\n","#Examples:\n","\n","- Base64: Commonly used to encode binary data (like images or files) in emails or JSON.\n","\n","#6.URL Encoding\n","\n","- Converts characters into a format that can be safely transmitted in URLs.\n","\n","#Example: Space becomes %20\n","\n","#Why is Encoding Important?\n","Compatibility: Ensures data can be read by different systems and software.\n","\n","Efficiency: Saves space and optimizes transmission time.\n","\n","Security: Some encodings are used in encryption or obfuscation processes.\n","\n","Accuracy: Prevents data corruption during storage or transmission."],"metadata":{"id":"v7ePQ8XzlLwA"}}]}